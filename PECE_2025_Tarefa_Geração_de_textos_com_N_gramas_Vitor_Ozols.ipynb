{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Descrição da tarefa\n",
        "\n",
        "Esta é uma tarefa de geração de texto (NLG - Natural Language Generation) baseada em modelos de n-gramas -- trigramas, especificamente.\n",
        "\n",
        "O corpus a ser utilizado é o conjunto da obra em prosa de Machado de Assis, contido no arquivo Machado.zip (na AlunoWeb), com um total aproximado de 1 milhão de palavras.\n",
        "\n",
        "As etapas do pré-processamento já estão prontas. Quando executadas, vão gerar uma lista de tokens já limpos, normalizados e com os delimitadores de sentenças ($<s>$ e $</s>$) incluídos.\n",
        "\n",
        "A partir daí, você deve:\n",
        "\n",
        "1. Gerar os trigramas. Use a função `zip()` para isso.\n",
        "\n",
        "2. Criar uma função para escolha de um trigrama de início de sentença com o qual você vai começar a geração do texto \"original\".\n",
        "\n",
        "3. Incluir novos trigramas até que seja encontrado um final \"natural\" de sentença ($</s>$).\n",
        "\n",
        "Neste notebook, além do pré-processamento, são incluídos outros fragmentos de código e sugestões pontuais para ajudar na solução da tarefa."
      ],
      "metadata": {
        "id": "RTAys18E_Hnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importação de módulos e de funções\n",
        "\n",
        "A maioria são recursos que já conhecemos bem. Algumas funções foram ligeiramente adaptadas por causa dos dados que serão utilizados no exercício."
      ],
      "metadata": {
        "id": "-Uyk70Ey7kne"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "efa784ed-62e4-4cf8-c508-d6cfe05d4166",
        "id": "LiojsmxwAqj2"
      },
      "source": [
        "import itertools\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Tokenização de sentenças\n",
        "def tokenizar_sentencas(txt: str) -> list:\n",
        "    txt = txt.replace('\\n', ' ')\n",
        "    return sent_tokenize(txt, language='portuguese')\n",
        "\n",
        "\n",
        "# Tokenização de palavras e símbolos\n",
        "def tokenizar(txt: str) -> list:\n",
        "    return word_tokenize(txt, language='portuguese')\n",
        "\n",
        "\n",
        "def limpar(lista: list) -> list:\n",
        "    return [i.lower() for i in lista if i.isalpha()]\n",
        "\n",
        "\n",
        "def achatar(lista: list) -> list:\n",
        "    return list(itertools.chain(*lista))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW_kqc67pjN2"
      },
      "source": [
        "# def ler(nome_arq):\n",
        "#     arq = open(nome_arq, 'r', encoding='latin-1')  # Essa foi a codificação usada nos textos\n",
        "#     corpus = arq.read()\n",
        "#     corpus = corpus.replace('\\n', ' ')\n",
        "#     arq.close()\n",
        "\n",
        "#     return corpus\n",
        "\n",
        "# A função de leitura de arquivos que conhecemos foi modificada\n",
        "# para usar a codificação \"latin-1\" usada nos textos do corpus Machado\n",
        "def ler(nome_arquivo) -> str:\n",
        "    with open(nome_arquivo, encoding='latin-1') as arquivo:\n",
        "        return arquivo.read()\n",
        "\n",
        "\n",
        "def delimitar_sents(lst_sents: list) -> list:\n",
        "    return [['<s>'] + i + ['</s>'] for i in lst_sents]\n",
        "\n",
        "\n",
        "\n",
        "# Este pipeline foi criado para facilitar a aplicação e ordenação das funções\n",
        "def pipeline(txt: str) -> list:\n",
        "    texto = ler(txt)\n",
        "    sentencas = tokenizar_sentencas(texto)\n",
        "    sentencas_de_tokens = [limpar(tokenizar(i)) for i in sentencas]\n",
        "    sentencas_delimitadas_de_tokens = delimitar_sents(sentencas_de_tokens)\n",
        "    listona_de_tokens = achatar(sentencas_delimitadas_de_tokens)\n",
        "\n",
        "    return listona_de_tokens"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregamento e processamento dos textos\n",
        "\n",
        "Antes de executar as células a seguir, você deve subir para o drive do Colab o arquivo 'Machado.zip', que está no Moodle."
      ],
      "metadata": {
        "id": "aVC59KMx8Rjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nova biblioteca: permite buscar facilmente nomes de arquivos no drive (HD ou virtual)\n",
        "import glob\n",
        "\n",
        "# Descompacta (no drive do Colab) o arquivo zipado\n",
        "!unzip 'Machado.zip' -d 'machado'\n",
        "\n",
        "# Recebe uma lista dos nomes de arquivos que foram descompactados na pasta 'machado'\n",
        "arqs = glob.glob('machado/*.txt')\n",
        "\n",
        "# Cria uma (grande) lista dos tokens de todas as obras\n",
        "lst_tokens = list()\n",
        "for arquivo in arqs:\n",
        "    print(f'Processando {arquivo}...')\n",
        "    tokens_da_obra = pipeline(arquivo)\n",
        "    lst_tokens.append(tokens_da_obra)\n",
        "\n",
        "# Até aqui, lst_tokens é uma lista de sublistas (cada lista é uma obra).\n",
        "# Será preciso transformá-la numa lista única, unidimensional:\n",
        "lst_tokens = achatar(lst_tokens)"
      ],
      "metadata": {
        "id": "Vy4m5vxyqzaQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d889adfd-f61b-4731-94dc-42fdf67845f1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Machado.zip\n",
            "  inflating: machado/A Mao e a Luva.txt  \n",
            "  inflating: machado/Casa Velha.txt  \n",
            "  inflating: machado/Contos Fluminenses.txt  \n",
            "  inflating: machado/Dom Casmurro.txt  \n",
            "  inflating: machado/Esau e Jaco.txt  \n",
            "  inflating: machado/Helena.txt      \n",
            "  inflating: machado/Historias Sem Data.txt  \n",
            "  inflating: machado/Historias da Meia-Noite.txt  \n",
            "  inflating: machado/Iaia Garcia.txt  \n",
            "  inflating: machado/Memorial de Aires.txt  \n",
            "  inflating: machado/Memorias Postumas de Bras Cubas.txt  \n",
            "  inflating: machado/Paginas Recolhidas.txt  \n",
            "  inflating: machado/Papeis Avulsos.txt  \n",
            "  inflating: machado/Quincas Borba.txt  \n",
            "  inflating: machado/Reliquias de Casa Velha.txt  \n",
            "  inflating: machado/Ressurreicao.txt  \n",
            "  inflating: machado/Varias Historias.txt  \n",
            "Processando machado/Memorial de Aires.txt...\n",
            "Processando machado/Quincas Borba.txt...\n",
            "Processando machado/A Mao e a Luva.txt...\n",
            "Processando machado/Historias Sem Data.txt...\n",
            "Processando machado/Papeis Avulsos.txt...\n",
            "Processando machado/Iaia Garcia.txt...\n",
            "Processando machado/Historias da Meia-Noite.txt...\n",
            "Processando machado/Reliquias de Casa Velha.txt...\n",
            "Processando machado/Dom Casmurro.txt...\n",
            "Processando machado/Esau e Jaco.txt...\n",
            "Processando machado/Paginas Recolhidas.txt...\n",
            "Processando machado/Memorias Postumas de Bras Cubas.txt...\n",
            "Processando machado/Ressurreicao.txt...\n",
            "Processando machado/Casa Velha.txt...\n",
            "Processando machado/Contos Fluminenses.txt...\n",
            "Processando machado/Varias Historias.txt...\n",
            "Processando machado/Helena.txt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Geração dos trigramas\n",
        "\n",
        "Você não precisará de nada além dos trigramas nesta tarefa. É só distribuir os tokens em uma grande lista de tuplas representando os trigramas. Use a função `zip()`."
      ],
      "metadata": {
        "id": "s9p6CQ6pCW-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams = list(zip(lst_tokens, lst_tokens[1:], lst_tokens[2:]))\n",
        "\n",
        "trigrams = list(set(trigrams))"
      ],
      "metadata": {
        "id": "8-2azV0VdBhq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.a. Filtragem dos trigramas\n",
        "\n",
        "Criados os trigramas, vamos:\n",
        "\n",
        "1. Filtrar os repetidos (sim, tem muitos!)\n",
        "\n",
        "2. Dividir os restantes em dois grupos:\n",
        "\n",
        "    a. Os que são iniciados por < s >. Chamemos a esse grupo de \"iniciais\".\n",
        "\n",
        "    b. Os demais serão chamados de \"sequentes\".\n",
        "\n",
        "3. Filtrar os sequentes para eliminar os que atrapalhariam a geração de boas sequências de texto: aqueles que têm o delimitador de fim de sentença (< /s >) em qualquer posição que não a última."
      ],
      "metadata": {
        "id": "RjCQOTP-C2ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iniciais = [tri for tri in trigrams if tri[0] == '<s>']\n",
        "\n",
        "sequentes = [tri for tri in trigrams if tri[0] != '<s>' and (tri[0] != '</s>' and tri[1] != '</s>')]"
      ],
      "metadata": {
        "id": "9K0uixJ-dFtk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.b. Criação de uma função de escolha aleatória\n",
        "\n",
        "Na geração de texto, é preciso mimetizar a criatividade com alguma aleatoriedade.\n",
        "\n",
        "Nosso gerador de texto precisará de duas funções de escolhas aleatórias:\n",
        "\n",
        "1. O trigrama com o qual será iniciada a sequência textual, a ser escolhido aleatoriamente da lista `iniciais`. Vamos atribuir o resultado dessa escolha a uma variável chamada `semente`.\n",
        "\n",
        "2. Os trigramas de continuidade da sequência textual, escolhidos a partir da lista `sequentes`. Atenção: antes de escolher o trigrama, será preciso gerar uma sublista a partir dos `sequentes` aplicando um filtro que selecione somente as tuplas cuja primeira palavra coincida com a última palavra da sequência já formada. Assim, por exemplo, se a sequência já formada for (< s >, 'bom', 'dia'), você deve filtrar a lista `sequentes` mantendo só as tuplas que comecem por 'dia'. Crie uma função chamada `sequencia` que receba a palavra final do texto já gerado e retorne uma nova tupla cuja primeira palavra coincida com a última do texto. Atenção de novo: essa tupla será formada pelo trigrama, mas não é interessante retornar a primeira palavra do trigrama, já que ela seria uma repetição da última palavra da tupla anterior. Retorne, somente, a segunda e a terceira palavras da tupla.\n",
        "\n",
        "Um modo prático de gerar escolhas aleatórias em Python é usar o módulo `random`. Ele traz um método `choice` que permite escolher aleatoriamente um item de uma lista qualquer. Segue uma ilustração de uso do método."
      ],
      "metadata": {
        "id": "r88SuOTtDED2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def sequencia(palavra_final, sequentes):\n",
        "    opcoes = [tri for tri in sequentes if tri[0] == palavra_final]\n",
        "    if not opcoes:\n",
        "        return None\n",
        "    escolhido = random.choice(opcoes)\n",
        "    return escolhido[1], escolhido[2]"
      ],
      "metadata": {
        "id": "rP23P0VBNjo_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Repetir a escolha de sequentes até chegar a um final de sentença\n",
        "\n",
        "A ideia é continuar a produzir texto \"original\" até que seja escolhida uma tupla com uma marcação de fim de sentença (< /s >).\n",
        "\n",
        "Como não se sabe quando o fim da sentença será encontrado, o modo mais simples de fazer isso é usando um loop com `while`."
      ],
      "metadata": {
        "id": "QLVIMjwHVu6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Só para deixar claro:\n",
        "\n",
        "1. Antes de entrar no loop, a `semente` deve iniciar o texto;\n",
        "\n",
        "2. Dentro do loop `while`, deve estar a chamada à função `sequencia`, que será repetida até que a condição de encontrar o final da sentença seja satisfeita;\n",
        "\n",
        "3. Sugestão: ao longo do processo, as tuplas selecionadas podem ser adicionadas a uma lista, que será exibida ao final."
      ],
      "metadata": {
        "id": "2t2w7n9gX-gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semente = random.choice(iniciais)\n",
        "texto_gerado = list(semente)\n",
        "\n",
        "while texto_gerado[-1] != '</s>':\n",
        "    proximo = sequencia(texto_gerado[-1], sequentes)\n",
        "\n",
        "    if proximo is None:\n",
        "        break\n",
        "\n",
        "    texto_gerado.extend(proximo)\n"
      ],
      "metadata": {
        "id": "bpChrn0jeuEq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Mostre o resultado para o mundo\n",
        "\n",
        "Chegou finalmente o momento de exibir a sequência de texto original gerada. Depois de tanto trabalho, vale a pena mostrar alguma coisa mais bonita do que uma lista de palavras, certo? Dê um jeito de fazer com que pareça um texto \"natural\", com letra maiúscula inicial, escrita cursiva (uma palavra depois da outra, com espaços em branco) e terminando com ponto final."
      ],
      "metadata": {
        "id": "PjQETFXLXRsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palavras = [token for token in texto_gerado if token not in ['<s>', '</s>']]\n",
        "\n",
        "frase = \" \".join(palavras)\n",
        "frase = frase.capitalize() + '.'\n",
        "\n",
        "print(frase)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-XJ_KfLrgmzN",
        "outputId": "da51be39-3dc9-4b2e-ef20-ee0505d56c3e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mamãe veja se não atribuía realmente o tinha a escritura qual destes e irão logo a criança teria já saído para um edifício leve um sorriso acanhado e lento quarenta cabeças e a responsabilidade da perda do meu tio sabe as coisas que serviam de resplendor maior da família poucas e silenciosas e não pude ter conhecido a mariposa que a viu o ano justamente em quando interrogativa queria ouvir o artigo político o ser tenta dominar a sensação de perdoar quer catar argumentos que empreguei algumas reflexões que sinto que me não arrependo de o comborço.\n"
          ]
        }
      ]
    }
  ]
}